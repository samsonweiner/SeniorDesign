{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module: model_select_eval_utils.py \n",
    "#### Author:  Joe Johnson\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "model_select_eval_utils.py is a utility module that facilitates conducting the model selection and evaluation\n",
    "process for any estimator (RandomForestRegressor, KNeighborRegressor, etc.) against a pre-existing set of \n",
    "5-fold CV splits files (train/test) for a dataset.  \n",
    "\n",
    "Simple calls to just three functions should be all that is required for the model selection and evaluation task, and for writing out the results to a report spreadsheet file, as explained in this document.  These instructions should apply for any estimator.\n",
    "\n",
    "The assumption of this utility is a nested approach to hyperparameter tuning where the models are tested in a 3-fold\n",
    "CV for each of the training sets in an outer 5-fold CV composition.  Refer to the approach outlined in Sebastian Raschka's web page for Machine Learning FAQ (approach 3):  https://sebastianraschka.com/faq/docs/evaluate-a-model.html.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "### 1.  Do the necessary imports.\n",
    "\n",
    "Suppose we wish to perform model selection for a RandomForestRegressor against a pre-existing collection of csv files storing the train/test splits for a 5-fold cross validation.  First, we import the necessary utility functions\n",
    "from the model_select_eval_utils module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohnson/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from model_select_eval_utils import load_data, select_model_one_fold, select_model_all_folds\n",
    "from model_select_eval_utils import evaluate_model, evaluate_bagging_model, write_model_results_to_file\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Initialize necessary variables.\n",
    "\n",
    "Next, we establish the source directory where the train/test 5-fold cv split files exist for the the dataset.  Also, we define three lists - one for the names of the input features, one for the names of the labels (usually just one field, here), and finally one for the names of the descriptive features.\n",
    "\n",
    "Finally, we instantiate the regressor object we wish to use in the model (here, we use the RandomForestRegressor, but this could be any valid estimator), and the parameter grid dictionary containing the range of settings over which we'd like to optimize.  See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"/Users/joejohnson/Documents/Research/Unilever/data/\" \\\n",
    "    + \"5_fold_stratified_splits_mcfann_problematic_records_false\"\n",
    "input_features = ['water','occlusive','emollient','elastomer','pres.']\n",
    "labels = ['SSNC absorb.']\n",
    "desc_features = ['id', 'description']\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "param_grid = [\n",
    "        {'bootstrap':[False],'max_depth':[2,4,6,8], 'max_features':[2,3,4,5], 'min_samples_leaf':[2,4,6,8],\n",
    "         'max_leaf_nodes': [4,6,8,10,12],'min_samples_split':[2,4,6,8,10],'n_estimators': [3]},\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Perform model selection using select_model_all_folds().\n",
    "\n",
    "Now, we can execute the select_model_all_folds() function.  This function walks through each training set of each of the 5 folds and finds the average test MSE from a 3-fold CV for each parameter combination, returning the optimal set for each of the 5 outer folds.  The hope is that if the model is stable, the same hyperparameter combination arises as the best model for all 5 folds.  (However, it is more likely they will *not* be exactly the same, but hopefully, similar enough so that some prudent choices can be made for optimal parameters.)  The return for this function is a dictionary that contains the information for each of the folds and can be queried to make a decision about which parameters to ultimately choose.\n",
    "\n",
    "Please refer to the function-level documentation in the model_select_eval_utilities.py file for further details on the required parameters for the select_model_all_folds() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = select_model_all_folds(source_directory, forest_reg, \n",
    "                              input_features, labels, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best_models variable holds a dictionary containing 5 dictionaries, each of which contains the information for each of the 5 best models from the 5 folds, as shown below.  This gives the user the ability to see which model performed best for each of the folds, and informs the decision about which set of hyperparameters to go with in the model evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run_time': 59.905066000000005, 'best_mse': -30.482957410706252, 'best_params': {'bootstrap': False, 'max_depth': 2, 'max_features': 2, 'max_leaf_nodes': 8, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 3}}\n",
      "{'run_time': 59.54335, 'best_mse': -19.143509989784743, 'best_params': {'bootstrap': False, 'max_depth': 6, 'max_features': 2, 'max_leaf_nodes': 10, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 3}}\n",
      "{'run_time': 58.901256000000004, 'best_mse': -24.779829136701643, 'best_params': {'bootstrap': False, 'max_depth': 2, 'max_features': 3, 'max_leaf_nodes': 6, 'min_samples_leaf': 6, 'min_samples_split': 2, 'n_estimators': 3}}\n",
      "{'run_time': 64.25101000000001, 'best_mse': -20.852899633019703, 'best_params': {'bootstrap': False, 'max_depth': 6, 'max_features': 2, 'max_leaf_nodes': 12, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 3}}\n",
      "{'run_time': 60.719201999999996, 'best_mse': -22.406084771744997, 'best_params': {'bootstrap': False, 'max_depth': 6, 'max_features': 2, 'max_leaf_nodes': 10, 'min_samples_leaf': 2, 'min_samples_split': 6, 'n_estimators': 3}}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(best_models['best_model_' + str(i+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that run times were very short - only about 1 minute per fold.  This is because this example uses a very small dataset consisting of only 11 records where for each fold, there were 9 in the training set and 2 in the test set.  (This example is only being used here for demonstration purposes.) Also, notice that the number of estimators = 3.  In a more realistic example, the run times will be considerably longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Evaluate the selected model using the evaluate_model() function.\n",
    "\n",
    "Now that we've run the model selection process, we can make a decision about which hyperparameters to use based on the results and run the function, evaluate_model(), to evaluate its performance on the outer train/test splits of the 5 fold CV splits.\n",
    "\n",
    "First, we instantiate the estimator with what appears to be a prudent choice for the hyperparameters, based on the results from the model selection step.  \n",
    "\n",
    "**Note: These hyperparameters shown below for the random forest are the same as those used by Shaoyan when building the model for predicting SSNC absorb from the input features** - 'water','occlusive','emollient','elastomer', and 'pres.'.  The results for the test sets of the 5-fold cv match those he produced in his analysis, as shown in the spreadsheet, 'Shaoyan_results_20190322_1.csv' for model, RF_1_1 for SSNC absorb.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg = RandomForestRegressor(random_state=42,\n",
    "                                   max_depth = 5,\n",
    "                                   max_features = 2,\n",
    "                                   max_leaf_nodes = 8,\n",
    "                                   min_samples_leaf = 3,\n",
    "                                   min_samples_split = 8,\n",
    "                                   n_estimators = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the evaluate_model() function with the estimator as a parameter, the list of input features, the labels list (Again, this should be a list of length 1 with only one field name in it, for now.)  And finally, a list containing the names of the descriptive fields should also be passed as an argument to this function.  (This last argument will help in the event we wish to look at the record-by-record detail of how the model performed.)\n",
    "\n",
    "Please refer to the function-level documentation in the model_select_eval_utilities.py file for further details on the required parameters for the evaluate_model() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = evaluate_model(forest_reg, source_directory, input_features, labels, desc_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate_model() function returns a dictionary giving the training and test MSE data for each of the 5 folds.  Furtehrmore, it also provides means of looking at the record-by-record detail of how the model's predicted value for the label compared with the actual value.\n",
    "\n",
    "First, let's look at the test MSE results for the 5 folds at the aggregate level:\n",
    "\n",
    "**Note:** __These results match those recorded by Shaoyan for the SSNC absorb model, as shown in the spreadsheet, 'Shaoyan_results_20190322_1.csv'__, both for the average test mse and for each fold of the 5-fold cv.  The average test mse of 21.0464 implies a 12.01% error when compared with the average SSNC absorb value of 38.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 21.04646388002244 : [14.30696373492298, 27.171424225673945, 16.265934266803875, 29.623633675384255, 17.864363497327147]\n"
     ]
    }
   ],
   "source": [
    "print(\"test: \" + str(model_results[\"mean_mse_test\"]) + \" : \" + str(model_results[\"fold_mses_test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also capture this information for the training sets, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Perform ad-hoc detailed analysis on problem folds by querying the model_results dictionary.\n",
    "\n",
    "Perhaps we're interested in looking at the results for the third fold in more detail.  We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                        description   water  \\\n",
      "0       NaN                                                PFP  46.097   \n",
      "1       NaN                                       1% PPAR Milk  70.820   \n",
      "2   ISRM 68                       Ponds Fine Pore Moisturizer   46.097   \n",
      "3   ISRM 14                                Pond's Age Defense   66.200   \n",
      "4    gjm647                                   match Olay silky  60.250   \n",
      "5   ISRM 37                           HUT 48 with 15% glycerin  56.965   \n",
      "6   ISRM 10  Dove Tinderbox (Intensive Cream, Nourishing Care)  54.280   \n",
      "7   ISRM 80                              \"63\" type at 0.74 H/V  43.075   \n",
      "8   ISRM 39        same as 38, with 0.2% Tween 40 added at end  58.508   \n",
      "9       NaN                                               DIFG  68.209   \n",
      "10  ISRM 30                                   53 with Silstar   45.360   \n",
      "11  ISRM 91                                       PDWN control  81.950   \n",
      "12  ISRM 76            silky with moisturization @10% glycerin  47.200   \n",
      "13  ISRM 40  TM15826164-2 silky with no silicone or particl...  86.250   \n",
      "14  ISRM 26                         15877-29 (batch#15608-185)  45.430   \n",
      "15      NaN                                              silky  63.730   \n",
      "16      NaN                        Intensive Firming Gel Cream  68.209   \n",
      "17      NaN                              Summer Revived Lotion  74.304   \n",
      "18      NaN                                        super silky  40.830   \n",
      "19      NaN                                      Fair & Lovely  74.890   \n",
      "20      NaN                                     Dove Body Silk  75.650   \n",
      "21  ISRM 84                   \"63\" type at 43% starting solids  43.075   \n",
      "\n",
      "    occlusive  emollient  elastomer  pres.      y     y_pred      diff  \\\n",
      "0        0.00       0.65       39.4  0.425  26.85  30.881108  4.031108   \n",
      "1        5.23       6.65        0.0  1.000  36.75  36.643213  0.106787   \n",
      "2        0.00       0.65       39.4  0.425  33.62  30.881108  2.738892   \n",
      "3        0.00       9.45        0.0  0.600  40.67  37.104634  3.565366   \n",
      "4        1.60       0.10       28.5  0.750  26.30  31.741712  5.441712   \n",
      "5        0.00       1.50       10.0  0.910  38.20  39.920274  1.720274   \n",
      "6       17.00      12.00        0.0  0.950  38.33  33.047489  5.282511   \n",
      "7        1.20       0.00       32.0  0.750  22.27  30.212122  7.942122   \n",
      "8        0.00       1.50       10.0  0.910  39.62  40.134070  0.514070   \n",
      "9        1.00       2.00        0.0  1.061  37.80  41.282365  3.482365   \n",
      "10       1.20       0.00       16.0  0.900  41.33  35.539001  5.790999   \n",
      "11       0.00       3.30        0.0  0.700  41.96  36.118511  5.841489   \n",
      "12       1.50       0.00       30.0  0.850  29.35  31.866108  2.516108   \n",
      "13       0.00      10.00        0.0  0.750  42.41  37.219558  5.190442   \n",
      "14       0.00       1.20       13.7  1.050  43.50  40.253007  3.246993   \n",
      "15       2.00       1.10       20.0  0.700  30.70  34.601568  3.901568   \n",
      "16       1.00       2.00        0.0  1.061  38.90  41.282365  2.382365   \n",
      "17       0.00       3.75        0.0  1.436  34.59  35.744853  1.154853   \n",
      "18       1.00       0.00       33.0  0.700  27.95  30.730748  2.780748   \n",
      "19       0.00       3.65        0.0  0.870  31.40  36.825777  5.425777   \n",
      "20       0.00       5.50        0.0  1.130  33.55  37.350579  3.800579   \n",
      "21       1.20       0.00       32.0  0.750  29.88  30.212122  0.332122   \n",
      "\n",
      "           se  \n",
      "0   16.249831  \n",
      "1    0.011403  \n",
      "2    7.501530  \n",
      "3   12.711835  \n",
      "4   29.612225  \n",
      "5    2.959342  \n",
      "6   27.904924  \n",
      "7   63.077303  \n",
      "8    0.264268  \n",
      "9   12.126869  \n",
      "10  33.535668  \n",
      "11  34.122992  \n",
      "12   6.330802  \n",
      "13  26.940691  \n",
      "14  10.542965  \n",
      "15  15.222233  \n",
      "16   5.675665  \n",
      "17   1.333686  \n",
      "18   7.732562  \n",
      "19  29.439054  \n",
      "20  14.444402  \n",
      "21   0.110305  \n"
     ]
    }
   ],
   "source": [
    "print(model_results['fold_3_detail_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to see at the detail level which records precipitated the test MSE value we saw at the aggregate level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Write results to file for reporting using the write_model_results_to_file() function.\n",
    "\n",
    "Finally, we can write out the performance measures (train/test MSE's for the model) to a csv file that's preformatted for insertion into a spreadsheet report by calling the write_model_results_to_file() function.  All we need to do is supply the model_results dictionary returned from the evaluate_model() step along with some descriptive info identifying the model, and some output file info.  \n",
    "\n",
    "Please refer to the function-level documentation in the model_select_eval_utilities.py file for further details on the required parameters for the write_model_results_to_file() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_model_results_to_file(\"RF_1_1\", \"rf_ssnc_absorb, mcfann problematic records false\", model_results, \"ssnc_absorb_test_results.csv\",\n",
    "                            append = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is more than one model whose results are to be written to the same file, we can call this function again with append = True to add rows to the existing file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
